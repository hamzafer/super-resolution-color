{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9,16,25,29,41,74,96,91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamzaz/miniconda3/envs/apw/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/hamzaz/miniconda3/envs/apw/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/hamzaz/miniconda3/envs/apw/lib/python3.11/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamzaz/miniconda3/envs/apw/lib/python3.11/site-packages/lpips/lpips.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR Image Dimensions: (678, 1020, 3), Generated Image Dimensions: (676, 1020, 3)\n",
      "Dimension mismatch: HR: (678, 1020, 3), Generated: (676, 1020, 3)\n",
      "SSIM Value: 0.651444762639113\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 111\u001b[0m\n\u001b[1;32m    103\u001b[0m models_dirs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBSRGAN\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflask/static/images/selected_256_BSRGAN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRealESRGAN\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflask/static/images/selected_256_RealESRGAN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResShift\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflask/static/images/selected_256_ResShift\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSwinIR\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflask/static/images/selected_256_SwinIR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    108\u001b[0m }\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# Calculate metrics\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m metrics \u001b[38;5;241m=\u001b[39m calculate_metrics(hr_dir, models_dirs)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Convert to DataFrame\u001b[39;00m\n\u001b[1;32m    114\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(metrics)\n",
      "Cell \u001b[0;32mIn[14], line 81\u001b[0m, in \u001b[0;36mcalculate_metrics\u001b[0;34m(hr_dir, models_dirs)\u001b[0m\n\u001b[1;32m     79\u001b[0m hr_tensor \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mToTensor()(Image\u001b[38;5;241m.\u001b[39mopen(hr_img_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     80\u001b[0m gen_tensor \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mToTensor()(Image\u001b[38;5;241m.\u001b[39mopen(gen_img_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 81\u001b[0m model_lpips\u001b[38;5;241m.\u001b[39mappend(lpips_fn(hr_tensor, gen_tensor)\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Calculate CLIPIQA\u001b[39;00m\n\u001b[1;32m     84\u001b[0m hr_clip_input \u001b[38;5;241m=\u001b[39m preprocess(Image\u001b[38;5;241m.\u001b[39mopen(hr_img_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/apw/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/apw/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/apw/lib/python3.11/site-packages/lpips/lpips.py:118\u001b[0m, in \u001b[0;36mLPIPS.forward\u001b[0;34m(self, in0, in1, retPerLayer, normalize)\u001b[0m\n\u001b[1;32m    115\u001b[0m     in1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m in1  \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# v0.0 - original release had a bug, where input was not scaled\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m in0_input, in1_input \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling_layer(in0), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling_layer(in1)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0.1\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m (in0, in1)\n\u001b[1;32m    119\u001b[0m outs0, outs1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet\u001b[38;5;241m.\u001b[39mforward(in0_input), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet\u001b[38;5;241m.\u001b[39mforward(in1_input)\n\u001b[1;32m    120\u001b[0m feats0, feats1, diffs \u001b[38;5;241m=\u001b[39m {}, {}, {}\n",
      "File \u001b[0;32m~/miniconda3/envs/apw/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/apw/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/apw/lib/python3.11/site-packages/lpips/lpips.py:154\u001b[0m, in \u001b[0;36mScalingLayer.forward\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inp):\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (inp \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshift) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import torch\n",
    "import lpips\n",
    "from clip import clip\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "def get_optimal_win_size(max_size, min_dim):\n",
    "    \"\"\"\n",
    "    Returns the largest odd win_size <= min(max_size, min_dim).\n",
    "    \"\"\"\n",
    "    for ws in range(min(max_size, min_dim), 2, -1):\n",
    "        if ws % 2 == 1:\n",
    "            return ws\n",
    "    return None\n",
    "\n",
    "def calculate_metrics(hr_dir, models_dirs):\n",
    "    results = []\n",
    "\n",
    "    # LPIPS setup\n",
    "    lpips_fn = lpips.LPIPS(net='alex')\n",
    "\n",
    "    # CLIPIQA setup\n",
    "    clip_model, preprocess = clip.load(\"ViT-B/32\", device=\"cuda\")\n",
    "\n",
    "    for model_name, model_dir in models_dirs.items():\n",
    "        model_psnr = []\n",
    "        model_ssim = []\n",
    "        model_lpips = []\n",
    "        model_clip = []\n",
    "\n",
    "        for img_name in os.listdir(model_dir):\n",
    "            hr_img_path = os.path.join(hr_dir, img_name)\n",
    "            gen_img_path = os.path.join(model_dir, img_name)\n",
    "\n",
    "            if not os.path.exists(hr_img_path):\n",
    "                continue\n",
    "\n",
    "            # Load images\n",
    "            hr_img = np.array(Image.open(hr_img_path).convert('RGB'))\n",
    "            gen_img = np.array(Image.open(gen_img_path).convert('RGB'))\n",
    "            print(f\"HR Image Dimensions: {hr_img.shape}, Generated Image Dimensions: {gen_img.shape}\")\n",
    "\n",
    "            if hr_img.shape != gen_img.shape:\n",
    "                print(f\"Dimension mismatch: HR: {hr_img.shape}, Generated: {gen_img.shape}\")\n",
    "                common_height = min(hr_img.shape[0], gen_img.shape[0])\n",
    "                common_width = min(hr_img.shape[1], gen_img.shape[1])\n",
    "\n",
    "                # Crop both images\n",
    "                hr_img = hr_img[:common_height, :common_width, :]\n",
    "                gen_img = gen_img[:common_height, :common_width, :]\n",
    "\n",
    "            # Calculate PSNR\n",
    "            model_psnr.append(psnr(hr_img, gen_img))\n",
    "\n",
    "            # Calculate SSIM\n",
    "            min_dim = min(hr_img.shape[0], hr_img.shape[1])\n",
    "            win_size = get_optimal_win_size(7, min_dim)\n",
    "            if win_size is None:\n",
    "                print(f\"Skipping SSIM for {img_name}: Images too small for SSIM calculation.\")\n",
    "                continue  # Skip SSIM calculation for this image pair\n",
    "\n",
    "            # Calculate SSIM with the determined win_size\n",
    "            try:\n",
    "                ssim_value = ssim(hr_img, gen_img, channel_axis=-1, win_size=win_size)\n",
    "                model_ssim.append(ssim_value)\n",
    "            except ValueError as ve:\n",
    "                print(f\"SSIM calculation failed for {img_name}: {ve}\")\n",
    "                continue\n",
    "\n",
    "            print(\"SSIM Value:\", ssim_value)\n",
    "\n",
    "            # Calculate LPIPS\n",
    "            hr_tensor = transforms.ToTensor()(Image.open(hr_img_path).convert('RGB')).unsqueeze(0).to('cuda')\n",
    "            gen_tensor = transforms.ToTensor()(Image.open(gen_img_path).convert('RGB')).unsqueeze(0).to('cuda')\n",
    "            model_lpips.append(lpips_fn(hr_tensor, gen_tensor).item())\n",
    "\n",
    "            # Calculate CLIPIQA\n",
    "            hr_clip_input = preprocess(Image.open(hr_img_path).convert('RGB')).unsqueeze(0).to('cuda')\n",
    "            gen_clip_input = preprocess(Image.open(gen_img_path).convert('RGB')).unsqueeze(0).to('cuda')\n",
    "            hr_features = clip_model.encode_image(hr_clip_input)\n",
    "            gen_features = clip_model.encode_image(gen_clip_input)\n",
    "            similarity = torch.cosine_similarity(hr_features, gen_features)\n",
    "            model_clip.append(similarity.item())\n",
    "\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"PSNR\": np.mean(model_psnr),\n",
    "            \"SSIM\": np.mean(model_ssim),\n",
    "            \"LPIPS\": np.mean(model_lpips),\n",
    "            \"CLIPIQA\": np.mean(model_clip),\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Directories\n",
    "hr_dir = \"dataset/DIV/selected/high\"\n",
    "models_dirs = {\n",
    "    \"BSRGAN\": \"flask/static/images/selected_256_BSRGAN\",\n",
    "    \"RealESRGAN\": \"flask/static/images/selected_256_RealESRGAN\",\n",
    "    \"ResShift\": \"flask/static/images/selected_256_ResShift\",\n",
    "    \"SwinIR\": \"flask/static/images/selected_256_SwinIR\",\n",
    "}\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = calculate_metrics(hr_dir, models_dirs)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(metrics)\n",
    "\n",
    "# Add summary row\n",
    "summary = {\n",
    "    \"Model\": \"Summary\",\n",
    "    \"PSNR\": df['PSNR'].mean(),\n",
    "    \"SSIM\": df['SSIM'].mean(),\n",
    "    \"LPIPS\": df['LPIPS'].mean(),\n",
    "    \"CLIPIQA\": df['CLIPIQA'].mean(),\n",
    "}\n",
    "df = df.append(summary, ignore_index=True)\n",
    "\n",
    "# Save and display results\n",
    "output_path = \"metrics_comparison.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
